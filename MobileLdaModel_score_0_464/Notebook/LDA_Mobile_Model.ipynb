{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-13T12:05:57.600822Z","iopub.execute_input":"2023-10-13T12:05:57.601228Z","iopub.status.idle":"2023-10-13T12:05:58.034843Z","shell.execute_reply.started":"2023-10-13T12:05:57.601195Z","shell.execute_reply":"2023-10-13T12:05:58.033625Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/mydata/MOBILE.xlsx\n/kaggle/input/mydata/pattern_replacements.json\n/kaggle/input/mydata/text2.txt\n/kaggle/input/mydata/query-hive-415.xlsx\n/kaggle/input/mydata/FIXE.xlsx\n/kaggle/input/mydata/cleaned_text_combined_mobile.pkl\n/kaggle/input/mydata/Mooli-Regular.ttf\n/kaggle/input/mydata/DATASET_HS_2023.xlsx\n/kaggle/input/mydata/Nuages_de_mots.xlsx\n/kaggle/input/mydata/frenchStopwords2.txt\n/kaggle/input/mydata/text.txt\n/kaggle/input/mydata/cleaned_text_combined.pkl\n/kaggle/input/mydata/text3.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install spacy-lefff","metadata":{"execution":{"iopub.status.busy":"2023-10-13T12:06:04.365002Z","iopub.execute_input":"2023-10-13T12:06:04.365488Z","iopub.status.idle":"2023-10-13T12:06:21.107552Z","shell.execute_reply.started":"2023-10-13T12:06:04.365457Z","shell.execute_reply":"2023-10-13T12:06:21.106336Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting spacy-lefff\n  Downloading spacy_lefff-0.5.1-py3-none-any.whl (2.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting black<23.0.0,>=22.6.0 (from spacy-lefff)\n  Downloading black-22.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting msgpack<0.6,>=0.3.0 (from spacy-lefff)\n  Downloading msgpack-0.5.6.tar.gz (138 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pluggy>=0.12.0 in /opt/conda/lib/python3.10/site-packages (from spacy-lefff) (1.0.0)\nRequirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (from spacy-lefff) (3.6.1)\nRequirement already satisfied: tqdm>=4.11.1 in /opt/conda/lib/python3.10/site-packages (from spacy-lefff) (4.66.1)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black<23.0.0,>=22.6.0->spacy-lefff) (8.1.7)\nRequirement already satisfied: mypy-extensions>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from black<23.0.0,>=22.6.0->spacy-lefff) (1.0.0)\nCollecting pathspec>=0.9.0 (from black<23.0.0,>=22.6.0->spacy-lefff)\n  Downloading pathspec-0.11.2-py3-none-any.whl (29 kB)\nRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black<23.0.0,>=22.6.0->spacy-lefff) (3.10.0)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black<23.0.0,>=22.6.0->spacy-lefff) (2.0.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (1.0.4)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (8.1.12)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (2.4.7)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (2.0.9)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (0.10.2)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (6.3.0)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (1.23.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (1.10.9)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (68.0.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (3.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy->spacy-lefff) (3.0.9)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->spacy-lefff) (4.6.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-lefff) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-lefff) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-lefff) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-lefff) (2023.7.22)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy->spacy-lefff) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy->spacy-lefff) (0.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy->spacy-lefff) (2.1.3)\nBuilding wheels for collected packages: msgpack\n  Building wheel for msgpack (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for msgpack: filename=msgpack-0.5.6-cp310-cp310-linux_x86_64.whl size=13845 sha256=9fd7974ab937e44f3f06ec723022b5bc488806d0ff6b6becaa57d68192c06321\n  Stored in directory: /root/.cache/pip/wheels/9d/27/cf/62a0a8f89d65ce9e28e3806cb3479f110d32a1c9cb0cdd6daf\nSuccessfully built msgpack\nInstalling collected packages: msgpack, pathspec, black, spacy-lefff\n  Attempting uninstall: msgpack\n    Found existing installation: msgpack 1.0.5\n    Uninstalling msgpack-1.0.5:\n      Successfully uninstalled msgpack-1.0.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndistributed 2023.9.0 requires msgpack>=1.0.0, but you have msgpack 0.5.6 which is incompatible.\nlibrosa 0.10.1 requires msgpack>=1.0, but you have msgpack 0.5.6 which is incompatible.\nray 2.5.1 requires msgpack<2.0.0,>=1.0.0, but you have msgpack 0.5.6 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed black-22.12.0 msgpack-0.5.6 pathspec-0.11.2 spacy-lefff-0.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!python -m spacy download fr_core_news_sm\n","metadata":{"execution":{"iopub.status.busy":"2023-10-13T12:06:21.109712Z","iopub.execute_input":"2023-10-13T12:06:21.110498Z","iopub.status.idle":"2023-10-13T12:06:50.281209Z","shell.execute_reply.started":"2023-10-13T12:06:21.110462Z","shell.execute_reply":"2023-10-13T12:06:50.280045Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting fr-core-news-sm==3.6.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.6.0/fr_core_news_sm-3.6.0-py3-none-any.whl (16.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /opt/conda/lib/python3.10/site-packages (from fr-core-news-sm==3.6.0) (3.6.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.0.4)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (8.1.12)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.4.7)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.0.9)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.10.2)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (6.3.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (4.66.1)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.23.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.10.9)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (68.0.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.0.9)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (4.6.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2023.7.22)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.1.1)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (8.1.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.1.3)\nInstalling collected packages: fr-core-news-sm\nSuccessfully installed fr-core-news-sm-3.6.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('fr_core_news_sm')\n","output_type":"stream"}]},{"cell_type":"code","source":"import gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom gensim.models import CoherenceModel\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nimport matplotlib.pyplot as plt\nimport os\nimport re\nimport spacy\nfrom PIL import Image\nimport nltk\nfrom concurrent.futures import ThreadPoolExecutor\nimport json","metadata":{"execution":{"iopub.status.busy":"2023-10-13T12:06:50.282912Z","iopub.execute_input":"2023-10-13T12:06:50.283221Z","iopub.status.idle":"2023-10-13T12:07:05.019954Z","shell.execute_reply.started":"2023-10-13T12:06:50.283194Z","shell.execute_reply":"2023-10-13T12:07:05.018635Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"}]},{"cell_type":"code","source":"nltk.download('popular')","metadata":{"execution":{"iopub.status.busy":"2023-10-13T12:07:05.021348Z","iopub.execute_input":"2023-10-13T12:07:05.022041Z","iopub.status.idle":"2023-10-13T12:07:06.283186Z","shell.execute_reply.started":"2023-10-13T12:07:05.022009Z","shell.execute_reply":"2023-10-13T12:07:06.282060Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading collection 'popular'\n[nltk_data]    | \n[nltk_data]    | Downloading package cmudict to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package gazetteers to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package inaugural to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package names to /usr/share/nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package shakespeare to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package shakespeare is already up-to-date!\n[nltk_data]    | Downloading package stopwords to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package omw to /usr/share/nltk_data...\n[nltk_data]    |   Package omw is already up-to-date!\n[nltk_data]    | Downloading package omw-1.4 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package omw-1.4 is already up-to-date!\n[nltk_data]    | Downloading package wordnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet2021 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package wordnet31 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package wordnet_ic to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to /usr/share/nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package snowball_data is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection popular\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"from spacy.language import Language\nfrom spacy_lefff import LefffLemmatizer\n\n@Language.factory('french_lemmatizer')\ndef create_french_lemmatizer(nlp, name):\n    return LefffLemmatizer()\nnlp = spacy.load('fr_core_news_sm')\nnlp.add_pipe('french_lemmatizer', name='lefff')","metadata":{"execution":{"iopub.status.busy":"2023-10-13T12:07:06.285420Z","iopub.execute_input":"2023-10-13T12:07:06.286154Z","iopub.status.idle":"2023-10-13T12:07:12.889856Z","shell.execute_reply.started":"2023-10-13T12:07:06.286114Z","shell.execute_reply":"2023-10-13T12:07:12.888502Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<spacy_lefff.lefff.LefffLemmatizer at 0x79ec19d6f910>"},"metadata":{}}]},{"cell_type":"code","source":"# Load the French stopwords\nwith open('/kaggle/input/mydata/frenchStopwords2.txt', 'r', encoding='utf-8') as file:\n    frenchStopwords = file.read().split('\\n')\n\n# Combine French stopwords with NLTK stopwords\nall_stopwords = frenchStopwords + stopwords.words('french')\n\n\n# Define a folder for uploading files\n#UPLOAD_FOLDER = 'uploads'\n#if not os.path.exists(UPLOAD_FOLDER):\n#    os.makedirs(UPLOAD_FOLDER)\n\n# Read the text file\n#with open('/kaggle/input/mydata/text2.txt', 'r', encoding='utf-8') as file:\n#    lines = file.readlines()\n\n# Load data from Excel file\nexcel_file_path = '/kaggle/input/mydata/MOBILE.xlsx'  # Specify the path to your Excel file\ndf = pd.read_excel(excel_file_path)\n\n# Define a list of patterns to search for and their corresponding replacement\n\n# Chargement des remplacements depuis le fichier JSON\nwith open('/kaggle/input/mydata/pattern_replacements.json', 'r', encoding='utf-8') as file:\n    pattern_replacements = json.load(file)\n\ndef merge_similar_phrases(text):\n    for pattern, replacement in pattern_replacements:\n        text = re.sub(pattern, replacement, text)\n    return text\n\n# Assuming your descriptions are in a column named 'description' in the Excel file\ndescriptions = df['Description'].tolist()\n\n# Function to clean and process text (similar to your previous code)\ndef clean_text(text):\n    # Apply the pattern replacements first\n    text_without_letter_apostrophe = re.sub(r\"\\b(?:[a-zA-Z])\\'\\b\", '', text)\n    text_lower = text_without_letter_apostrophe.lower()\n    cleantext = merge_similar_phrases(text_lower)\n    # Tokenization (dividing the text into words)\n    words = nltk.word_tokenize(cleantext)  # Convert to lowercase\n    # Remove punctuation, stopwords, and non-alphabetic words\n    cleaned_words = [word for word in words if word not in all_stopwords and word.isalpha()]\n    # Apply the pattern replacements to the cleaned words\n    cleaned_words = [merge_similar_phrases(word) for word in cleaned_words]\n    # Lemmatization\n    doc = nlp(\" \".join(cleaned_words))\n    lemmatized_words = [token.lemma_ for token in doc]\n    # Return a list of lemmatized words instead of a single string\n    return lemmatized_words\n\n\n\n#documents = text\n\n# Divisez les lignes en lots (par exemple, 10 lignes par lot)\nbatch_size = 10000\ndescription_batches = [descriptions[i:i + batch_size] for i in range(0, len(descriptions), batch_size)]\n\n# Fonction pour traiter une ligne\ndef process_line(lines):\n    cleaned_text_list = []\n    for line in lines:\n         if isinstance(line, str):\n                # Nettoyez et traitez la ligne comme un document LDA\n                cleaned_text = clean_text(line)\n                cleaned_text_list.append(cleaned_text)\n    return cleaned_text_list\n\n\n# Créez un ThreadPoolExecutor avec un certain nombre de threads\nnum_threads = 3  # Vous pouvez ajuster le nombre de threads en fonction de votre matériel\nwith ThreadPoolExecutor(max_workers=num_threads) as executor:\n    # Traitez chaque lot de lignes en parallèle\n    cleaned_text_list = list(executor.map(process_line, description_batches))\n\n# Combinez les résultats en une seule liste de texte nettoyé pour LDA\ncleaned_text_combined = [text for batch in cleaned_text_list for text in batch]\n","metadata":{"execution":{"iopub.status.busy":"2023-10-13T12:07:12.891537Z","iopub.execute_input":"2023-10-13T12:07:12.891896Z","iopub.status.idle":"2023-10-13T12:46:38.367634Z","shell.execute_reply.started":"2023-10-13T12:07:12.891867Z","shell.execute_reply":"2023-10-13T12:46:38.366448Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\n# Convertissez votre texte nettoyé en une liste de listes de mots (tokens)\n#cleanedTextList = [cleanedText.split()]\n\n# Créez le dictionnaire à partir de la liste de listes de mots\ndictionary = gensim.corpora.Dictionary(cleaned_text_combined)\n\n#dictionary.filter_extremes(no_below=15, no_above=0.6, keep_n=1000)\nbow_corpus = [dictionary.doc2bow(doc) for doc in cleaned_text_combined]\nlda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=12, id2word=dictionary, passes=25)\ntopics = []\nfor idx, topic in lda_model.print_topics(-1) :\n    print(\"Topic: {} -> Words: {}\".format(idx, topic))\n    topics.append(topic)\n\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=cleaned_text_combined, dictionary=dictionary)\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('Coherence Score: ', coherence_lda)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-13T12:49:09.808879Z","iopub.execute_input":"2023-10-13T12:49:09.809790Z","iopub.status.idle":"2023-10-13T12:56:59.061435Z","shell.execute_reply.started":"2023-10-13T12:49:09.809754Z","shell.execute_reply":"2023-10-13T12:56:59.060487Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Topic: 0 -> Words: 0.078*\"constater\" + 0.076*\"connecter\" + 0.068*\"barr\" + 0.055*\"souscription\" + 0.050*\"forfaire\" + 0.048*\"autre\" + 0.046*\"changer\" + 0.044*\"numéro\" + 0.041*\"région\" + 0.036*\"bâtiment\"\nTopic: 1 -> Words: 0.178*\"crédit\" + 0.103*\"recevoir\" + 0.102*\"perdu\" + 0.079*\"bonus\" + 0.073*\"commun\" + 0.056*\"recu\" + 0.036*\"rechargemer\" + 0.035*\"recharg\" + 0.027*\"sms\" + 0.027*\"region\"\nTopic: 2 -> Words: 0.178*\"message\" + 0.143*\"service\" + 0.048*\"desactiver\" + 0.039*\"activation\" + 0.031*\"recevoir\" + 0.025*\"carte\" + 0.024*\"recharger\" + 0.022*\"activer\" + 0.019*\"désactiver\" + 0.018*\"oranger\"\nTopic: 3 -> Words: 0.112*\"canal\" + 0.091*\"om\" + 0.081*\"identification\" + 0.080*\"compte\" + 0.065*\"code\" + 0.057*\"sim\" + 0.037*\"oranger\" + 0.023*\"money\" + 0.021*\"app\" + 0.020*\"imsi\"\nTopic: 4 -> Words: 0.117*\"souci\" + 0.078*\"souscrir\" + 0.059*\"reseau\" + 0.048*\"illimi\" + 0.032*\"connexion\" + 0.030*\"signal\" + 0.027*\"mixel\" + 0.026*\"cnx\" + 0.023*\"barre\" + 0.022*\"box\"\nTopic: 5 -> Words: 0.288*\"pas\" + 0.076*\"vrai\" + 0.065*\"acheter\" + 0.045*\"pass\" + 0.039*\"internet\" + 0.035*\"achat\" + 0.026*\"mois\" + 0.023*\"type\" + 0.019*\"cours\" + 0.019*\"terme\"\nTopic: 6 -> Words: 0.065*\"village\" + 0.058*\"info\" + 0.038*\"mobile\" + 0.034*\"information\" + 0.030*\"offre\" + 0.026*\"informer\" + 0.025*\"vouloir\" + 0.024*\"passer\" + 0.022*\"connaitre\" + 0.018*\"seck\"\nTopic: 7 -> Words: 0.105*\"bénéficiaire\" + 0.089*\"faux\" + 0.081*\"montant\" + 0.077*\"contact\" + 0.063*\"seddo\" + 0.059*\"erreur\" + 0.047*\"sim\" + 0.037*\"envoyer\" + 0.033*\"appel\" + 0.033*\"solde\"\nTopic: 8 -> Words: 0.154*\"point\" + 0.135*\"sargal\" + 0.070*\"favori\" + 0.046*\"consulter\" + 0.044*\"objet\" + 0.028*\"narrive\" + 0.024*\"convertir\" + 0.024*\"pièce\" + 0.021*\"echec\" + 0.020*\"solde\"\nTopic: 9 -> Words: 0.226*\"transaction\" + 0.075*\"erreur\" + 0.072*\"om\" + 0.061*\"achat\" + 0.055*\"faire\" + 0.046*\"effectuer\" + 0.035*\"annuler\" + 0.035*\"transfert\" + 0.030*\"typ\" + 0.029*\"ussd\"\nTopic: 10 -> Words: 0.104*\"demande\" + 0.043*\"réseau\" + 0.037*\"hors\" + 0.031*\"info\" + 0.029*\"nombre\" + 0.024*\"box\" + 0.024*\"num\" + 0.020*\"etat\" + 0.017*\"fiab\" + 0.014*\"invite\"\nTopic: 11 -> Words: 0.248*\"credit\" + 0.104*\"sos\" + 0.093*\"promo\" + 0.076*\"activer\" + 0.071*\"détaillant\" + 0.049*\"perte\" + 0.034*\"vocal\" + 0.028*\"messagerie\" + 0.013*\"mauvais\" + 0.013*\"souscription\"\nCoherence Score:  0.4378846753978378\n","output_type":"stream"}]},{"cell_type":"code","source":"# Enregistrez le modèle LDA dans un fichier\nmodel_path = \"/kaggle/working/model_mobile_12topics_464\"\nlda_model.save(model_path)\n\n# Vous pouvez également sauvegarder le dictionnaire si nécessaire\ndictionary_path = \"/kaggle/working/dictionary_mobile464\"\ndictionary.save(dictionary_path)\n\n\n\n# Pour charger le modèle plus tard, vous pouvez utiliser la commande suivante\n#loaded_lda_model = gensim.models.LdaMulticore.load(model_path)\n\n# Pour charger le dictionnaire\n#loaded_dictionary = gensim.corpora.Dictionary.load(dictionary_path)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n# Spécifiez le chemin du fichier dans lequel vous souhaitez enregistrer la liste\noutput_file_path = '/kaggle/working/cleaned_text_combined_mobile.pkl'\n\n# Enregistrez la liste dans le fichier\nwith open(output_file_path, 'wb') as file:\n    pickle.dump(cleaned_text_combined, file)\n\nprint(f'La liste a été enregistrée dans {output_file_path}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n# Spécifiez le chemin du fichier contenant la liste sauvegardée\ninput_file_path = '/kaggle/input/mydata/cleaned_text_combined_mobile.pkl'\n\n# Chargez la liste à partir du fichier\nwith open(input_file_path, 'rb') as file:\n    cleaned_text_combined = pickle.load(file)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:25:15.594537Z","iopub.execute_input":"2023-10-12T10:25:15.595019Z","iopub.status.idle":"2023-10-12T10:25:16.877352Z","shell.execute_reply.started":"2023-10-12T10:25:15.594982Z","shell.execute_reply":"2023-10-12T10:25:16.875473Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from gensim import corpora\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:13:44.042023Z","iopub.execute_input":"2023-10-12T10:13:44.042378Z","iopub.status.idle":"2023-10-12T10:13:44.047790Z","shell.execute_reply.started":"2023-10-12T10:13:44.042352Z","shell.execute_reply":"2023-10-12T10:13:44.046515Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Créez des bigrammes\nbigram = Phrases(cleaned_text_combined, min_count=50, threshold=100)\n\n# Créez des trigrammes en utilisant les bigrammes\n#trigram = Phrases(bigram[cleaned_text_combined], threshold=100)\n\n# Appliquez les bigrammes et trigrammes à vos documents\nbigram_mod = Phraser(bigram)\n#trigram_mod = Phraser(trigram)\n\n# Appliquez les bigrammes et trigrammes à chaque document\nbigram_documents = [bigram_mod[doc] for doc in cleaned_text_combined]\n#trigram_documents = [trigram_mod[bigram_mod[doc]] for doc in cleaned_text_combined]\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:52:46.226447Z","iopub.execute_input":"2023-10-12T10:52:46.226871Z","iopub.status.idle":"2023-10-12T10:52:49.332878Z","shell.execute_reply.started":"2023-10-12T10:52:46.226832Z","shell.execute_reply":"2023-10-12T10:52:49.331769Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Créez un nouveau dictionnaire à partir des trigrammes\ndictionary_with_bigrams = corpora.Dictionary(bigram_documents)\n\n# Créez un nouveau corpus en utilisant le dictionnaire avec les trigrammes\nbow_corpus_with_bigrams = [dictionary_with_bigrams.doc2bow(doc) for doc in bigram_documents]\n\n# Entraînez votre modèle LDA avec le corpus et le dictionnaire mis à jour\nlda_model_with_bigrams = gensim.models.LdaMulticore(bow_corpus_with_bigrams, num_topics=12, id2word=dictionary_with_bigrams, passes=25)\n\ntopics = []\nfor idx, topic in lda_model_with_bigrams.print_topics(-1) :\n    print(\"Topic: {} -> Words: {}\".format(idx, topic))\n    topics.append(topic)\n\ncoherence_model_lda_with_bigrams = CoherenceModel(model=lda_model_with_bigrams, texts=bigram_documents, dictionary=dictionary_with_bigrams)\ncoherence_lda_with_bigrams = coherence_model_lda_with_bigrams.get_coherence()\nprint('Coherence Score: ', coherence_lda_with_bigrams)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:32:59.037029Z","iopub.execute_input":"2023-10-12T11:32:59.037449Z","iopub.status.idle":"2023-10-12T11:40:51.209135Z","shell.execute_reply.started":"2023-10-12T11:32:59.037404Z","shell.execute_reply":"2023-10-12T11:40:51.207675Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Topic: 0 -> Words: 0.113*\"pas\" + 0.111*\"constater\" + 0.078*\"forfaire\" + 0.071*\"terme\" + 0.059*\"épuiser\" + 0.044*\"cours\" + 0.042*\"epuise\" + 0.034*\"barre\" + 0.027*\"vite\" + 0.021*\"so\"\nTopic: 1 -> Words: 0.240*\"pas\" + 0.100*\"acheter\" + 0.089*\"achat\" + 0.077*\"sos\" + 0.069*\"credit\" + 0.050*\"souscrir\" + 0.034*\"internet\" + 0.033*\"mois\" + 0.028*\"crédit\" + 0.027*\"type\"\nTopic: 2 -> Words: 0.258*\"transaction\" + 0.134*\"bénéficiaire\" + 0.089*\"erreur\" + 0.077*\"om\" + 0.065*\"faire\" + 0.046*\"annuler\" + 0.046*\"transfert\" + 0.039*\"effectuer\" + 0.020*\"changer\" + 0.018*\"montant\"\nTopic: 3 -> Words: 0.054*\"pass\" + 0.052*\"illimi\" + 0.047*\"box\" + 0.039*\"pas\" + 0.033*\"connexion\" + 0.025*\"cnx\" + 0.022*\"signal\" + 0.021*\"recharger\" + 0.021*\"modem\" + 0.020*\"souscrir\"\nTopic: 4 -> Words: 0.094*\"souci\" + 0.072*\"recevoir\" + 0.056*\"bonus\" + 0.055*\"reseau\" + 0.046*\"réseau\" + 0.036*\"appel\" + 0.023*\"ref\" + 0.021*\"remboursement\" + 0.019*\"rechargemer\" + 0.017*\"connaitre\"\nTopic: 5 -> Words: 0.155*\"service\" + 0.077*\"concerner\" + 0.074*\"barr\" + 0.056*\"desactiver\" + 0.025*\"flash\" + 0.020*\"désactiver\" + 0.018*\"factur\" + 0.016*\"acce\" + 0.016*\"zone\" + 0.015*\"payer\"\nTopic: 6 -> Words: 0.185*\"point\" + 0.173*\"sargal\" + 0.111*\"promo\" + 0.063*\"consulter\" + 0.058*\"autre\" + 0.033*\"convertir\" + 0.016*\"echange\" + 0.014*\"acceder\" + 0.013*\"echanger\" + 0.011*\"effectue\"\nTopic: 7 -> Words: 0.090*\"om\" + 0.073*\"canal\" + 0.069*\"code\" + 0.061*\"compte\" + 0.052*\"oranger\" + 0.046*\"ussd\" + 0.040*\"recu\" + 0.034*\"money\" + 0.030*\"souscription\" + 0.027*\"achat\"\nTopic: 8 -> Words: 0.127*\"demande\" + 0.080*\"info\" + 0.070*\"activer\" + 0.057*\"typ\" + 0.046*\"mixel\" + 0.027*\"etat\" + 0.026*\"num\" + 0.023*\"activation\" + 0.022*\"souscrir\" + 0.017*\"app\"\nTopic: 9 -> Words: 0.126*\"credit\" + 0.112*\"crédit\" + 0.105*\"vrai\" + 0.091*\"perdu\" + 0.052*\"favori\" + 0.043*\"village\" + 0.043*\"perte\" + 0.037*\"o\" + 0.035*\"sold\" + 0.034*\"echec\"\nTopic: 10 -> Words: 0.079*\"faux\" + 0.069*\"contact\" + 0.069*\"montant\" + 0.063*\"sim\" + 0.060*\"seddo\" + 0.050*\"erreur\" + 0.038*\"message\" + 0.036*\"solde\" + 0.034*\"objet\" + 0.034*\"envoyer\"\nTopic: 11 -> Words: 0.168*\"connecter\" + 0.117*\"appel\" + 0.057*\"information\" + 0.051*\"offre\" + 0.042*\"suivre\" + 0.039*\"émettre\" + 0.037*\"narrive\" + 0.026*\"consommation\" + 0.022*\"parvenir\" + 0.019*\"kaolack\"\nCoherence Score:  0.41089572167577915\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\nfrom sklearn.model_selection import ParameterSampler\n\n# Créez le dictionnaire à partir de la liste de listes de mots\ndictionary = gensim.corpora.Dictionary(cleaned_text_combined)\n\ndictionary.filter_extremes(no_below=15, no_above=0.2, keep_n=100)\nbow_corpus = [dictionary.doc2bow(doc) for doc in cleaned_text_combined]\nlda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=12, id2word=dictionary, passes=100)\ntopics = []\n\n# Définir l'espace des hyperparamètres que vous souhaitez optimiser\nparam_space = {\n    'num_topics': [5, 10, 15, 20],\n    'passes': [50, 100, 150],\n    'alpha': ['auto', 0.01, 0.1, 1.0],\n    'eta': ['auto', 0.01, 0.1, 1.0],\n}\n\n# Nombre d'itérations de recherche aléatoire\nn_iterations = 10\n\n# Liste pour stocker les résultats\nresults = []\n\n# Effectuer la recherche aléatoire\nfor _ in range(n_iterations):\n    # Échantillon aléatoire des hyperparamètres\n    params = random.choice(list(ParameterSampler(param_space, n_iter=1)))\n\n    # Utiliser les hyperparamètres pour ajuster un modèle LDA\n    lda_model = gensim.models.LdaModel(bow_corpus, id2word=dictionary, **params)\n\n    # Évaluer le modèle (par exemple, avec la cohérence des sujets)\n    coherence_model_lda = CoherenceModel(model=lda_model, texts=cleaned_text_combined, dictionary=dictionary)\n    coherence_lda = coherence_model_lda.get_coherence()\n\n    # Enregistrer les résultats\n    results.append({'params': params, 'coherence': coherence_lda})\n\n# Triez les résultats par cohérence décroissante\nresults.sort(key=lambda x: x['coherence'], reverse=True)\n\n# Affichez les résultats triés\nfor result in results:\n    print(\"Hyperparameters:\", result['params'])\n    print(\"Coherence Score:\", result['coherence'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}