{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-13T08:37:17.801689Z","iopub.execute_input":"2023-10-13T08:37:17.802183Z","iopub.status.idle":"2023-10-13T08:37:18.191754Z","shell.execute_reply.started":"2023-10-13T08:37:17.802139Z","shell.execute_reply":"2023-10-13T08:37:18.190598Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/mydata/MOBILE.xlsx\n/kaggle/input/mydata/pattern_replacements.json\n/kaggle/input/mydata/text2.txt\n/kaggle/input/mydata/query-hive-415.xlsx\n/kaggle/input/mydata/FIXE.xlsx\n/kaggle/input/mydata/cleaned_text_combined_mobile.pkl\n/kaggle/input/mydata/Mooli-Regular.ttf\n/kaggle/input/mydata/DATASET_HS_2023.xlsx\n/kaggle/input/mydata/Nuages_de_mots.xlsx\n/kaggle/input/mydata/frenchStopwords2.txt\n/kaggle/input/mydata/text.txt\n/kaggle/input/mydata/cleaned_text_combined.pkl\n/kaggle/input/mydata/text3.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install spacy-lefff","metadata":{"execution":{"iopub.status.busy":"2023-10-13T08:37:18.193422Z","iopub.execute_input":"2023-10-13T08:37:18.193954Z","iopub.status.idle":"2023-10-13T08:37:34.769698Z","shell.execute_reply.started":"2023-10-13T08:37:18.193878Z","shell.execute_reply":"2023-10-13T08:37:34.768379Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting spacy-lefff\n  Downloading spacy_lefff-0.5.1-py3-none-any.whl (2.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hCollecting black<23.0.0,>=22.6.0 (from spacy-lefff)\n  Downloading black-22.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting msgpack<0.6,>=0.3.0 (from spacy-lefff)\n  Downloading msgpack-0.5.6.tar.gz (138 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pluggy>=0.12.0 in /opt/conda/lib/python3.10/site-packages (from spacy-lefff) (1.0.0)\nRequirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (from spacy-lefff) (3.6.1)\nRequirement already satisfied: tqdm>=4.11.1 in /opt/conda/lib/python3.10/site-packages (from spacy-lefff) (4.66.1)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black<23.0.0,>=22.6.0->spacy-lefff) (8.1.7)\nRequirement already satisfied: mypy-extensions>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from black<23.0.0,>=22.6.0->spacy-lefff) (1.0.0)\nCollecting pathspec>=0.9.0 (from black<23.0.0,>=22.6.0->spacy-lefff)\n  Downloading pathspec-0.11.2-py3-none-any.whl (29 kB)\nRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black<23.0.0,>=22.6.0->spacy-lefff) (3.10.0)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black<23.0.0,>=22.6.0->spacy-lefff) (2.0.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (1.0.4)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (8.1.12)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (2.4.7)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (2.0.9)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (0.10.2)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (6.3.0)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (1.23.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (1.10.9)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (68.0.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy->spacy-lefff) (3.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy->spacy-lefff) (3.0.9)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->spacy-lefff) (4.6.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-lefff) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-lefff) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-lefff) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-lefff) (2023.7.22)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy->spacy-lefff) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy->spacy-lefff) (0.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy->spacy-lefff) (2.1.3)\nBuilding wheels for collected packages: msgpack\n  Building wheel for msgpack (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for msgpack: filename=msgpack-0.5.6-cp310-cp310-linux_x86_64.whl size=13845 sha256=355382c399a502f0d45735f249e02c025d8e5c6bf1596019261d4ac14fcb1b8e\n  Stored in directory: /root/.cache/pip/wheels/9d/27/cf/62a0a8f89d65ce9e28e3806cb3479f110d32a1c9cb0cdd6daf\nSuccessfully built msgpack\nInstalling collected packages: msgpack, pathspec, black, spacy-lefff\n  Attempting uninstall: msgpack\n    Found existing installation: msgpack 1.0.5\n    Uninstalling msgpack-1.0.5:\n      Successfully uninstalled msgpack-1.0.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndistributed 2023.9.0 requires msgpack>=1.0.0, but you have msgpack 0.5.6 which is incompatible.\nlibrosa 0.10.1 requires msgpack>=1.0, but you have msgpack 0.5.6 which is incompatible.\nray 2.5.1 requires msgpack<2.0.0,>=1.0.0, but you have msgpack 0.5.6 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed black-22.12.0 msgpack-0.5.6 pathspec-0.11.2 spacy-lefff-0.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!python -m spacy download fr_core_news_sm\n","metadata":{"execution":{"iopub.status.busy":"2023-10-13T08:37:34.771377Z","iopub.execute_input":"2023-10-13T08:37:34.771714Z","iopub.status.idle":"2023-10-13T08:38:02.204020Z","shell.execute_reply.started":"2023-10-13T08:37:34.771684Z","shell.execute_reply":"2023-10-13T08:38:02.203018Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting fr-core-news-sm==3.6.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.6.0/fr_core_news_sm-3.6.0-py3-none-any.whl (16.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /opt/conda/lib/python3.10/site-packages (from fr-core-news-sm==3.6.0) (3.6.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.0.4)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (8.1.12)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.4.7)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.0.9)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.10.2)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (6.3.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (4.66.1)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.23.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.10.9)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (68.0.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.0.9)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (4.6.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2023.7.22)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.1.1)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (8.1.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.1.3)\nInstalling collected packages: fr-core-news-sm\nSuccessfully installed fr-core-news-sm-3.6.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('fr_core_news_sm')\n","output_type":"stream"}]},{"cell_type":"code","source":"import gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom gensim.models import CoherenceModel\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nimport matplotlib.pyplot as plt\nimport os\nimport re\nimport spacy\nfrom PIL import Image\nimport nltk\nfrom concurrent.futures import ThreadPoolExecutor\nimport json","metadata":{"execution":{"iopub.status.busy":"2023-10-13T08:38:02.206862Z","iopub.execute_input":"2023-10-13T08:38:02.207961Z","iopub.status.idle":"2023-10-13T08:38:16.264653Z","shell.execute_reply.started":"2023-10-13T08:38:02.207922Z","shell.execute_reply":"2023-10-13T08:38:16.263483Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"}]},{"cell_type":"code","source":"nltk.download('popular')","metadata":{"execution":{"iopub.status.busy":"2023-10-13T08:38:16.266029Z","iopub.execute_input":"2023-10-13T08:38:16.266704Z","iopub.status.idle":"2023-10-13T08:38:17.864598Z","shell.execute_reply.started":"2023-10-13T08:38:16.266673Z","shell.execute_reply":"2023-10-13T08:38:17.863697Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading collection 'popular'\n[nltk_data]    | \n[nltk_data]    | Downloading package cmudict to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package gazetteers to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package inaugural to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package names to /usr/share/nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package shakespeare to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package shakespeare is already up-to-date!\n[nltk_data]    | Downloading package stopwords to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package omw to /usr/share/nltk_data...\n[nltk_data]    |   Package omw is already up-to-date!\n[nltk_data]    | Downloading package omw-1.4 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package omw-1.4 is already up-to-date!\n[nltk_data]    | Downloading package wordnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet2021 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package wordnet31 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package wordnet_ic to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to /usr/share/nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package snowball_data is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection popular\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"from spacy.language import Language\nfrom spacy_lefff import LefffLemmatizer\n\n@Language.factory('french_lemmatizer')\ndef create_french_lemmatizer(nlp, name):\n    return LefffLemmatizer()\nnlp = spacy.load('fr_core_news_sm')\nnlp.add_pipe('french_lemmatizer', name='lefff')","metadata":{"execution":{"iopub.status.busy":"2023-10-13T08:38:17.865717Z","iopub.execute_input":"2023-10-13T08:38:17.866108Z","iopub.status.idle":"2023-10-13T08:38:24.301742Z","shell.execute_reply.started":"2023-10-13T08:38:17.866081Z","shell.execute_reply":"2023-10-13T08:38:24.300649Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<spacy_lefff.lefff.LefffLemmatizer at 0x7c7493193220>"},"metadata":{}}]},{"cell_type":"code","source":"# Load the French stopwords\nwith open('/kaggle/input/mydata/frenchStopwords2.txt', 'r', encoding='utf-8') as file:\n    frenchStopwords = file.read().split('\\n')\n\n# Combine French stopwords with NLTK stopwords\nall_stopwords = frenchStopwords + stopwords.words('french')\n\n\n# Define a folder for uploading files\n#UPLOAD_FOLDER = 'uploads'\n#if not os.path.exists(UPLOAD_FOLDER):\n#    os.makedirs(UPLOAD_FOLDER)\n\n# Read the text file\n#with open('/kaggle/input/mydata/text2.txt', 'r', encoding='utf-8') as file:\n#    lines = file.readlines()\n\n# Load data from Excel file\nexcel_file_path = '/kaggle/input/mydata/MOBILE.xlsx'  # Specify the path to your Excel file\ndf = pd.read_excel(excel_file_path)\n\n# Define a list of patterns to search for and their corresponding replacement\n\n# Chargement des remplacements depuis le fichier JSON\nwith open('/kaggle/input/mydata/pattern_replacements.json', 'r', encoding='utf-8') as file:\n    pattern_replacements = json.load(file)\n\ndef merge_similar_phrases(text):\n    for pattern, replacement in pattern_replacements:\n        text = re.sub(pattern, replacement, text)\n    return text\n\n# Assuming your descriptions are in a column named 'description' in the Excel file\ndescriptions = df['Description'].tolist()\n\n# Function to clean and process text (similar to your previous code)\ndef clean_text(text):\n    # Apply the pattern replacements first\n    text_without_letter_apostrophe = re.sub(r\"\\b(?:[a-zA-Z])\\'\\b\", '', text)\n    text_lower = text_without_letter_apostrophe.lower()\n    cleantext = merge_similar_phrases(text_lower)\n    # Tokenization (dividing the text into words)\n    words = nltk.word_tokenize(cleantext)  # Convert to lowercase\n    # Remove punctuation, stopwords, and non-alphabetic words\n    cleaned_words = [word for word in words if word not in all_stopwords and word.isalpha()]\n    # Lemmatization\n    doc = nlp(\" \".join(cleaned_words))\n    lemmatized_words = [token.lemma_ for token in doc]\n    # Return a list of lemmatized words instead of a single string\n    return lemmatized_words\n\n\n\n#documents = text\n\n# Divisez les lignes en lots (par exemple, 10 lignes par lot)\nbatch_size = 10000\ndescription_batches = [descriptions[i:i + batch_size] for i in range(0, len(descriptions), batch_size)]\n\n# Fonction pour traiter une ligne\ndef process_line(lines):\n    cleaned_text_list = []\n    for line in lines:\n         if isinstance(line, str):\n                # Nettoyez et traitez la ligne comme un document LDA\n                cleaned_text = clean_text(line)\n                cleaned_text_list.append(cleaned_text)\n    return cleaned_text_list\n\n\n# Créez un ThreadPoolExecutor avec un certain nombre de threads\nnum_threads = 3  # Vous pouvez ajuster le nombre de threads en fonction de votre matériel\nwith ThreadPoolExecutor(max_workers=num_threads) as executor:\n    # Traitez chaque lot de lignes en parallèle\n    cleaned_text_list = list(executor.map(process_line, description_batches))\n\n# Combinez les résultats en une seule liste de texte nettoyé pour LDA\ncleaned_text_combined = [text for batch in cleaned_text_list for text in batch]\n","metadata":{"execution":{"iopub.status.busy":"2023-10-13T08:38:24.303334Z","iopub.execute_input":"2023-10-13T08:38:24.303664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n# Spécifiez le chemin du fichier dans lequel vous souhaitez enregistrer la liste\noutput_file_path = '/kaggle/working/cleaned_text_combined_mobile.pkl'\n\n# Enregistrez la liste dans le fichier\nwith open(output_file_path, 'wb') as file:\n    pickle.dump(cleaned_text_combined, file)\n\nprint(f'La liste a été enregistrée dans {output_file_path}')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T08:47:08.591305Z","iopub.execute_input":"2023-10-12T08:47:08.592000Z","iopub.status.idle":"2023-10-12T08:47:09.241643Z","shell.execute_reply.started":"2023-10-12T08:47:08.591966Z","shell.execute_reply":"2023-10-12T08:47:09.240022Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Enregistrez la liste dans le fichier\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m----> 7\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43mcleaned_text_combined\u001b[49m, file)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLa liste a été enregistrée dans \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'cleaned_text_combined' is not defined"],"ename":"NameError","evalue":"name 'cleaned_text_combined' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import pickle\n# Spécifiez le chemin du fichier contenant la liste sauvegardée\ninput_file_path = '/kaggle/input/mydata/cleaned_text_combined_mobile.pkl'\n\n# Chargez la liste à partir du fichier\nwith open(input_file_path, 'rb') as file:\n    cleaned_text_combined = pickle.load(file)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:25:15.594537Z","iopub.execute_input":"2023-10-12T10:25:15.595019Z","iopub.status.idle":"2023-10-12T10:25:16.877352Z","shell.execute_reply.started":"2023-10-12T10:25:15.594982Z","shell.execute_reply":"2023-10-12T10:25:16.875473Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\n# Convertissez votre texte nettoyé en une liste de listes de mots (tokens)\n#cleanedTextList = [cleanedText.split()]\n\n# Créez le dictionnaire à partir de la liste de listes de mots\ndictionary = gensim.corpora.Dictionary(cleaned_text_combined)\n\n#dictionary.filter_extremes(no_below=15, no_above=0.6, keep_n=1000)\nbow_corpus = [dictionary.doc2bow(doc) for doc in cleaned_text_combined]\nlda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=12, id2word=dictionary, passes=25)\ntopics = []\nfor idx, topic in lda_model.print_topics(-1) :\n    print(\"Topic: {} -> Words: {}\".format(idx, topic))\n    topics.append(topic)\n\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=cleaned_text_combined, dictionary=dictionary)\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('Coherence Score: ', coherence_lda)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T09:05:04.001478Z","iopub.execute_input":"2023-10-12T09:05:04.002459Z","iopub.status.idle":"2023-10-12T09:12:47.151182Z","shell.execute_reply.started":"2023-10-12T09:05:04.002382Z","shell.execute_reply":"2023-10-12T09:12:47.149721Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Topic: 0 -> Words: 0.214*\"contact\" + 0.117*\"service\" + 0.082*\"appel\" + 0.075*\"activer\" + 0.054*\"commun\" + 0.048*\"souscrir\" + 0.045*\"mixel\" + 0.043*\"numéro\" + 0.040*\"desactiver\" + 0.023*\"quartier\"\nTopic: 1 -> Words: 0.124*\"connecter\" + 0.103*\"nombre\" + 0.078*\"autre\" + 0.076*\"changer\" + 0.057*\"profil\" + 0.045*\"ba\" + 0.042*\"application\" + 0.033*\"utiliser\" + 0.031*\"cours\" + 0.031*\"voir\"\nTopic: 2 -> Words: 0.122*\"promo\" + 0.069*\"favori\" + 0.066*\"perte\" + 0.065*\"credit\" + 0.063*\"transfert\" + 0.035*\"sms\" + 0.027*\"argent\" + 0.027*\"désactiver\" + 0.026*\"crédit\" + 0.023*\"souscription\"\nTopic: 3 -> Words: 0.159*\"demande\" + 0.064*\"illimi\" + 0.056*\"hors\" + 0.037*\"num\" + 0.032*\"receiver\" + 0.031*\"etat\" + 0.031*\"remboursement\" + 0.029*\"sender\" + 0.027*\"info\" + 0.021*\"mpec\"\nTopic: 4 -> Words: 0.217*\"pas\" + 0.093*\"internet\" + 0.085*\"forfaire\" + 0.070*\"constater\" + 0.070*\"terme\" + 0.059*\"épuiser\" + 0.043*\"epuise\" + 0.042*\"pass\" + 0.027*\"vite\" + 0.023*\"education\"\nTopic: 5 -> Words: 0.143*\"transaction\" + 0.088*\"bénéficiaire\" + 0.074*\"seddo\" + 0.074*\"faux\" + 0.073*\"om\" + 0.072*\"montant\" + 0.041*\"sim\" + 0.039*\"vrai\" + 0.039*\"solde\" + 0.038*\"envoyer\"\nTopic: 6 -> Words: 0.086*\"recevoir\" + 0.083*\"message\" + 0.077*\"crédit\" + 0.065*\"oranger\" + 0.064*\"perdu\" + 0.051*\"recu\" + 0.049*\"réseau\" + 0.049*\"credit\" + 0.046*\"détaillant\" + 0.044*\"souscription\"\nTopic: 7 -> Words: 0.083*\"souci\" + 0.042*\"reseau\" + 0.042*\"concerner\" + 0.032*\"box\" + 0.030*\"message\" + 0.026*\"connexion\" + 0.024*\"région\" + 0.021*\"bâtiment\" + 0.021*\"département\" + 0.021*\"signal\"\nTopic: 8 -> Words: 0.104*\"pas\" + 0.101*\"erreur\" + 0.081*\"achat\" + 0.060*\"transaction\" + 0.056*\"faire\" + 0.050*\"acheter\" + 0.050*\"objet\" + 0.046*\"canal\" + 0.045*\"effectuer\" + 0.029*\"ussd\"\nTopic: 9 -> Words: 0.136*\"credit\" + 0.118*\"compte\" + 0.115*\"sos\" + 0.088*\"om\" + 0.039*\"crédit\" + 0.037*\"souscrir\" + 0.032*\"app\" + 0.030*\"recharger\" + 0.029*\"pièce\" + 0.026*\"flash\"\nTopic: 10 -> Words: 0.085*\"identification\" + 0.078*\"sim\" + 0.062*\"info\" + 0.050*\"cni\" + 0.037*\"activation\" + 0.036*\"constater\" + 0.035*\"svp\" + 0.027*\"barre\" + 0.027*\"informer\" + 0.024*\"rc\"\nTopic: 11 -> Words: 0.197*\"point\" + 0.170*\"sargal\" + 0.058*\"consulter\" + 0.049*\"fournir\" + 0.032*\"favori\" + 0.030*\"convertir\" + 0.028*\"appel\" + 0.027*\"suivre\" + 0.026*\"information\" + 0.026*\"f\"\nCoherence Score:  0.43833852727966\n","output_type":"stream"}]},{"cell_type":"code","source":"from gensim import corpora\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:13:44.042023Z","iopub.execute_input":"2023-10-12T10:13:44.042378Z","iopub.status.idle":"2023-10-12T10:13:44.047790Z","shell.execute_reply.started":"2023-10-12T10:13:44.042352Z","shell.execute_reply":"2023-10-12T10:13:44.046515Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Créez des bigrammes\nbigram = Phrases(cleaned_text_combined, min_count=50, threshold=100)\n\n# Créez des trigrammes en utilisant les bigrammes\n#trigram = Phrases(bigram[cleaned_text_combined], threshold=100)\n\n# Appliquez les bigrammes et trigrammes à vos documents\nbigram_mod = Phraser(bigram)\n#trigram_mod = Phraser(trigram)\n\n# Appliquez les bigrammes et trigrammes à chaque document\nbigram_documents = [bigram_mod[doc] for doc in cleaned_text_combined]\n#trigram_documents = [trigram_mod[bigram_mod[doc]] for doc in cleaned_text_combined]\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T10:52:46.226447Z","iopub.execute_input":"2023-10-12T10:52:46.226871Z","iopub.status.idle":"2023-10-12T10:52:49.332878Z","shell.execute_reply.started":"2023-10-12T10:52:46.226832Z","shell.execute_reply":"2023-10-12T10:52:49.331769Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Créez un nouveau dictionnaire à partir des trigrammes\ndictionary_with_bigrams = corpora.Dictionary(bigram_documents)\n\n# Créez un nouveau corpus en utilisant le dictionnaire avec les trigrammes\nbow_corpus_with_bigrams = [dictionary_with_bigrams.doc2bow(doc) for doc in bigram_documents]\n\n# Entraînez votre modèle LDA avec le corpus et le dictionnaire mis à jour\nlda_model_with_bigrams = gensim.models.LdaMulticore(bow_corpus_with_bigrams, num_topics=12, id2word=dictionary_with_bigrams, passes=25)\n\ntopics = []\nfor idx, topic in lda_model_with_bigrams.print_topics(-1) :\n    print(\"Topic: {} -> Words: {}\".format(idx, topic))\n    topics.append(topic)\n\ncoherence_model_lda_with_bigrams = CoherenceModel(model=lda_model_with_bigrams, texts=bigram_documents, dictionary=dictionary_with_bigrams)\ncoherence_lda_with_bigrams = coherence_model_lda_with_bigrams.get_coherence()\nprint('Coherence Score: ', coherence_lda_with_bigrams)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:32:59.037029Z","iopub.execute_input":"2023-10-12T11:32:59.037449Z","iopub.status.idle":"2023-10-12T11:40:51.209135Z","shell.execute_reply.started":"2023-10-12T11:32:59.037404Z","shell.execute_reply":"2023-10-12T11:40:51.207675Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Topic: 0 -> Words: 0.113*\"pas\" + 0.111*\"constater\" + 0.078*\"forfaire\" + 0.071*\"terme\" + 0.059*\"épuiser\" + 0.044*\"cours\" + 0.042*\"epuise\" + 0.034*\"barre\" + 0.027*\"vite\" + 0.021*\"so\"\nTopic: 1 -> Words: 0.240*\"pas\" + 0.100*\"acheter\" + 0.089*\"achat\" + 0.077*\"sos\" + 0.069*\"credit\" + 0.050*\"souscrir\" + 0.034*\"internet\" + 0.033*\"mois\" + 0.028*\"crédit\" + 0.027*\"type\"\nTopic: 2 -> Words: 0.258*\"transaction\" + 0.134*\"bénéficiaire\" + 0.089*\"erreur\" + 0.077*\"om\" + 0.065*\"faire\" + 0.046*\"annuler\" + 0.046*\"transfert\" + 0.039*\"effectuer\" + 0.020*\"changer\" + 0.018*\"montant\"\nTopic: 3 -> Words: 0.054*\"pass\" + 0.052*\"illimi\" + 0.047*\"box\" + 0.039*\"pas\" + 0.033*\"connexion\" + 0.025*\"cnx\" + 0.022*\"signal\" + 0.021*\"recharger\" + 0.021*\"modem\" + 0.020*\"souscrir\"\nTopic: 4 -> Words: 0.094*\"souci\" + 0.072*\"recevoir\" + 0.056*\"bonus\" + 0.055*\"reseau\" + 0.046*\"réseau\" + 0.036*\"appel\" + 0.023*\"ref\" + 0.021*\"remboursement\" + 0.019*\"rechargemer\" + 0.017*\"connaitre\"\nTopic: 5 -> Words: 0.155*\"service\" + 0.077*\"concerner\" + 0.074*\"barr\" + 0.056*\"desactiver\" + 0.025*\"flash\" + 0.020*\"désactiver\" + 0.018*\"factur\" + 0.016*\"acce\" + 0.016*\"zone\" + 0.015*\"payer\"\nTopic: 6 -> Words: 0.185*\"point\" + 0.173*\"sargal\" + 0.111*\"promo\" + 0.063*\"consulter\" + 0.058*\"autre\" + 0.033*\"convertir\" + 0.016*\"echange\" + 0.014*\"acceder\" + 0.013*\"echanger\" + 0.011*\"effectue\"\nTopic: 7 -> Words: 0.090*\"om\" + 0.073*\"canal\" + 0.069*\"code\" + 0.061*\"compte\" + 0.052*\"oranger\" + 0.046*\"ussd\" + 0.040*\"recu\" + 0.034*\"money\" + 0.030*\"souscription\" + 0.027*\"achat\"\nTopic: 8 -> Words: 0.127*\"demande\" + 0.080*\"info\" + 0.070*\"activer\" + 0.057*\"typ\" + 0.046*\"mixel\" + 0.027*\"etat\" + 0.026*\"num\" + 0.023*\"activation\" + 0.022*\"souscrir\" + 0.017*\"app\"\nTopic: 9 -> Words: 0.126*\"credit\" + 0.112*\"crédit\" + 0.105*\"vrai\" + 0.091*\"perdu\" + 0.052*\"favori\" + 0.043*\"village\" + 0.043*\"perte\" + 0.037*\"o\" + 0.035*\"sold\" + 0.034*\"echec\"\nTopic: 10 -> Words: 0.079*\"faux\" + 0.069*\"contact\" + 0.069*\"montant\" + 0.063*\"sim\" + 0.060*\"seddo\" + 0.050*\"erreur\" + 0.038*\"message\" + 0.036*\"solde\" + 0.034*\"objet\" + 0.034*\"envoyer\"\nTopic: 11 -> Words: 0.168*\"connecter\" + 0.117*\"appel\" + 0.057*\"information\" + 0.051*\"offre\" + 0.042*\"suivre\" + 0.039*\"émettre\" + 0.037*\"narrive\" + 0.026*\"consommation\" + 0.022*\"parvenir\" + 0.019*\"kaolack\"\nCoherence Score:  0.41089572167577915\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Enregistrez le modèle LDA dans un fichier\nmodel_path = \"/kaggle/working/lda_model_with_bigrams\"\nlda_model_with_bigrams.save(model_path)\n\n# Vous pouvez également sauvegarder le dictionnaire si nécessaire\n#dictionary_path = \"/kaggle/working/dictionarymobile439\"\n#dictionary.save(dictionary_path)\n\n# Pour charger le modèle plus tard, vous pouvez utiliser la commande suivante\n#loaded_lda_model = gensim.models.LdaMulticore.load(model_path)\n\n# Pour charger le dictionnaire\n#loaded_dictionary = gensim.corpora.Dictionary.load(dictionary_path)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T11:13:50.950818Z","iopub.execute_input":"2023-10-12T11:13:50.951224Z","iopub.status.idle":"2023-10-12T11:13:51.012640Z","shell.execute_reply.started":"2023-10-12T11:13:50.951194Z","shell.execute_reply":"2023-10-12T11:13:51.011733Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import random\nfrom sklearn.model_selection import ParameterSampler\n\n# Créez le dictionnaire à partir de la liste de listes de mots\ndictionary = gensim.corpora.Dictionary(cleaned_text_combined)\n\ndictionary.filter_extremes(no_below=15, no_above=0.2, keep_n=100)\nbow_corpus = [dictionary.doc2bow(doc) for doc in cleaned_text_combined]\nlda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=12, id2word=dictionary, passes=100)\ntopics = []\n\n# Définir l'espace des hyperparamètres que vous souhaitez optimiser\nparam_space = {\n    'num_topics': [5, 10, 15, 20],\n    'passes': [50, 100, 150],\n    'alpha': ['auto', 0.01, 0.1, 1.0],\n    'eta': ['auto', 0.01, 0.1, 1.0],\n}\n\n# Nombre d'itérations de recherche aléatoire\nn_iterations = 10\n\n# Liste pour stocker les résultats\nresults = []\n\n# Effectuer la recherche aléatoire\nfor _ in range(n_iterations):\n    # Échantillon aléatoire des hyperparamètres\n    params = random.choice(list(ParameterSampler(param_space, n_iter=1)))\n\n    # Utiliser les hyperparamètres pour ajuster un modèle LDA\n    lda_model = gensim.models.LdaModel(bow_corpus, id2word=dictionary, **params)\n\n    # Évaluer le modèle (par exemple, avec la cohérence des sujets)\n    coherence_model_lda = CoherenceModel(model=lda_model, texts=cleaned_text_combined, dictionary=dictionary)\n    coherence_lda = coherence_model_lda.get_coherence()\n\n    # Enregistrer les résultats\n    results.append({'params': params, 'coherence': coherence_lda})\n\n# Triez les résultats par cohérence décroissante\nresults.sort(key=lambda x: x['coherence'], reverse=True)\n\n# Affichez les résultats triés\nfor result in results:\n    print(\"Hyperparameters:\", result['params'])\n    print(\"Coherence Score:\", result['coherence'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the French stopwords\nwith open('/kaggle/input/mydata/frenchStopwords2.txt', 'r', encoding='utf-8') as file:\n    frenchStopwords = file.read().split('\\n')\n\n# Combine French stopwords with NLTK stopwords\nall_stopwords = frenchStopwords + stopwords.words('french')\n\n\n# Define a folder for uploading files\n#UPLOAD_FOLDER = 'uploads'\n#if not os.path.exists(UPLOAD_FOLDER):\n#    os.makedirs(UPLOAD_FOLDER)\n\n# Read the text file\n#with open('/kaggle/input/mydata/text2.txt', 'r', encoding='utf-8') as file:\n#    lines = file.readlines()\n\n# Load data from Excel file\nexcel_file_path = '/kaggle/input/mydata/MOBILE.xlsx'  # Specify the path to your Excel file\ndf = pd.read_excel(excel_file_path)\n\n# Define a list of patterns to search for and their corresponding replacement\n\npattern_replacements = [\n\n    (r'\\bpositionn\\b', 'positionner'),\n\n    (r'\\brechargemer\\b', 'rechargement'),\n\n    (r'\\bbloqu\\b', 'bloque'),\n\n    (r'\\brechar\\b', 'retrait'),\n\n    (r'\\bretraire\\b', 'retrait'),\n\n    (r'\\bstart\\b', 'star'),\n\n    (r'\\bsouscrir\\b', 'souscription'),\n\n    (r'\\bsauvegard\\b', 'sauvegarde'),\n\n    (r'\\brepertoir\\b', 'repertoire'),\n\n    (r'\\brecu\\b', 'reçu'),\n\n    (r'\\bperdre\\b', 'perdu'),  # Replace \"perdre\" with \"perdu\"\n    (r'\\bcredit\\b', 'crédit'),  # Replace \"perdre\" with \"perdu\"\n\n    (r'\\bcas\\b', 'case'),\n\n    (r'\\bpas\\b', 'pass'),\n\n    (r'\\bsouscrir\\b', 'souscription'),\n\n    (r'\\billimi\\b', 'illimix'),\n\n    (r'\\bperte\\b', 'perdu'),   # Replace \"perte\" with \"perdu\"\n\n    (r'\\bperdre\\u00A0', 'perdu'),  # Replace \"perdre\" with \"perdu\"\n\n    (r'\\bperte\\u00A0', 'perdu'),   # Replace \"perte\" with \"perdu\"\n    (r'\\bbénéficiaire faux\\b', 'faux bénéficiaire'),\n    (r'\\bfaire sos\\b', 'souscrire sos'),\n    (r'\\bsouscrir sos\\b', 'souscrire sos'),\n    (r'\\bcnx\\b', 'connexion'),\n    (r'\\bpasser\\b', 'pass'),\n    (r'\\bchanger mot\\b', 'changer mdp'),\n    # Add more patterns as needed\n]\n\ndef merge_similar_phrases(text):\n    for pattern, replacement in pattern_replacements:\n        text = re.sub(pattern, replacement, text)\n    return text\n\n# Assuming your descriptions are in a column named 'description' in the Excel file\ndescriptions = df['Description'].tolist()\n\n# Function to clean and process text (similar to your previous code)\ndef clean_text(text):\n    # Apply the pattern replacements first\n    text = merge_similar_phrases(text)\n    \n    text_without_letter_apostrophe = re.sub(r\"\\b(?:[a-zA-Z])\\'\\b\", '', text)\n    # Tokenization (dividing the text into words)\n    words = nltk.word_tokenize(text_without_letter_apostrophe.lower())  # Convert to lowercase\n    # Remove punctuation, stopwords, and non-alphabetic words\n    cleaned_words = [word for word in words if word not in all_stopwords and word.isalpha()]\n    # Lemmatization\n    doc = nlp(\" \".join(cleaned_words))\n    lemmatized_words = [token.lemma_ for token in doc]\n    # Return a list of lemmatized words instead of a single string\n    return lemmatized_words\n\n\n\n#documents = text\n\n# Divisez les lignes en lots (par exemple, 10 lignes par lot)\nbatch_size = 10000\ndescription_batches = [descriptions[i:i + batch_size] for i in range(0, len(descriptions), batch_size)]\n\n# Fonction pour traiter une ligne\ndef process_line(lines):\n    cleaned_text_list = []\n    for line in lines:\n         if isinstance(line, str):\n                # Nettoyez et traitez la ligne comme un document LDA\n                cleaned_text = clean_text(line)\n                cleaned_text_list.append(cleaned_text)\n    return cleaned_text_list\n\n\n# Créez un ThreadPoolExecutor avec un certain nombre de threads\nnum_threads = 3  # Vous pouvez ajuster le nombre de threads en fonction de votre matériel\nwith ThreadPoolExecutor(max_workers=num_threads) as executor:\n    # Traitez chaque lot de lignes en parallèle\n    cleaned_text_list = list(executor.map(process_line, description_batches))\n\n# Combinez les résultats en une seule liste de texte nettoyé pour LDA\ncleaned_text_combined = [text for batch in cleaned_text_list for text in batch]\n","metadata":{"execution":{"iopub.status.busy":"2023-10-06T07:42:50.634840Z","iopub.execute_input":"2023-10-06T07:42:50.635322Z","iopub.status.idle":"2023-10-06T07:43:01.900180Z","shell.execute_reply.started":"2023-10-06T07:42:50.635288Z","shell.execute_reply":"2023-10-06T07:43:01.899165Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyldavis in /opt/conda/lib/python3.10/site-packages (3.2.2)\nRequirement already satisfied: wheel>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from pyldavis) (0.40.0)\nRequirement already satisfied: numpy>=1.9.2 in /opt/conda/lib/python3.10/site-packages (from pyldavis) (1.23.5)\nRequirement already satisfied: scipy>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from pyldavis) (1.11.2)\nRequirement already satisfied: joblib>=0.8.4 in /opt/conda/lib/python3.10/site-packages (from pyldavis) (1.3.2)\nRequirement already satisfied: jinja2>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from pyldavis) (3.1.2)\nRequirement already satisfied: numexpr in /opt/conda/lib/python3.10/site-packages (from pyldavis) (2.8.5)\nRequirement already satisfied: future in /opt/conda/lib/python3.10/site-packages (from pyldavis) (0.18.3)\nRequirement already satisfied: funcy in /opt/conda/lib/python3.10/site-packages (from pyldavis) (2.0)\nRequirement already satisfied: pandas>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from pyldavis) (2.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.7.2->pyldavis) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.17.0->pyldavis) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.17.0->pyldavis) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.17.0->pyldavis) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=0.17.0->pyldavis) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Enregistrez le modèle LDA dans un fichier\nmodel_path = \"/kaggle/working/model_mobile10topics_435\"\nlda_model.save(model_path)\n\n# Vous pouvez également sauvegarder le dictionnaire si nécessaire\ndictionary_path = \"/kaggle/working/dictionarymobile\"\ndictionary.save(dictionary_path)\n\n# Pour charger le modèle plus tard, vous pouvez utiliser la commande suivante\n#loaded_lda_model = gensim.models.LdaMulticore.load(model_path)\n\n# Pour charger le dictionnaire\n#loaded_dictionary = gensim.corpora.Dictionary.load(dictionary_path)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T15:17:38.879311Z","iopub.execute_input":"2023-10-11T15:17:38.879757Z","iopub.status.idle":"2023-10-11T15:17:38.931913Z","shell.execute_reply.started":"2023-10-11T15:17:38.879726Z","shell.execute_reply":"2023-10-11T15:17:38.930820Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pyLDAvis.gensim as gensimvis\nimport pyLDAvis\n","metadata":{"execution":{"iopub.status.busy":"2023-10-06T07:45:51.814218Z","iopub.execute_input":"2023-10-06T07:45:51.814855Z","iopub.status.idle":"2023-10-06T07:45:51.838403Z","shell.execute_reply.started":"2023-10-06T07:45:51.814827Z","shell.execute_reply":"2023-10-06T07:45:51.837454Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Créez la visualisation\nlda_display = gensimvis.prepare(lda_model, bow_corpus, dictionary)\n\n# Affichez la visualisation dans votre notebook\npyLDAvis.display(lda_display)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-06T07:49:15.100254Z","iopub.execute_input":"2023-10-06T07:49:15.100651Z","iopub.status.idle":"2023-10-06T07:49:15.283854Z","shell.execute_reply.started":"2023-10-06T07:49:15.100622Z","shell.execute_reply":"2023-10-06T07:49:15.281095Z"},"trusted":true},"execution_count":11,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Créez la visualisation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m lda_display \u001b[38;5;241m=\u001b[39m \u001b[43mgensimvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlda_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbow_corpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Affichez la visualisation dans votre notebook\u001b[39;00m\n\u001b[1;32m      5\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39mdisplay(lda_display)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyLDAvis/gensim.py:124\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03mthe data structures needed for the visualization.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03mSee `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m opts \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mmerge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvis_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyLDAvis/_prepare.py:440\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Quick fix for red bar width bug.  We calculate the\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# term frequencies internally, using the topic term distributions and the\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# topic frequencies, rather than using the user-supplied term frequencies.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# For a detailed discussion, see: https://github.com/cpsievert/LDAvis/pull/41\u001b[39;00m\n\u001b[1;32m    438\u001b[0m term_frequency \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(term_topic_freq, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 440\u001b[0m topic_info \u001b[38;5;241m=\u001b[39m \u001b[43m_topic_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_term_dists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic_proportion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mterm_frequency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm_topic_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m token_table \u001b[38;5;241m=\u001b[39m _token_table(topic_info, term_topic_freq, vocab, term_frequency, start_index)\n\u001b[1;32m    444\u001b[0m topic_coordinates \u001b[38;5;241m=\u001b[39m _topic_coordinates(mds, topic_term_dists, topic_proportion, start_index)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyLDAvis/_prepare.py:248\u001b[0m, in \u001b[0;36m_topic_info\u001b[0;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs, start_index)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# Order the terms for the \"default\" view by decreasing saliency:\u001b[39;00m\n\u001b[1;32m    241\u001b[0m default_term_info \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaliency\u001b[39m\u001b[38;5;124m'\u001b[39m: saliency,\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTerm\u001b[39m\u001b[38;5;124m'\u001b[39m: vocab,\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFreq\u001b[39m\u001b[38;5;124m'\u001b[39m: term_frequency,\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal\u001b[39m\u001b[38;5;124m'\u001b[39m: term_frequency,\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDefault\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m    247\u001b[0m default_term_info \u001b[38;5;241m=\u001b[39m \u001b[43mdefault_term_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m--> 248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msaliency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msaliency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Rounding Freq and Total to integer values to match LDAvis code:\u001b[39;00m\n\u001b[1;32m    250\u001b[0m default_term_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFreq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloor(default_term_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFreq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","\u001b[0;31mTypeError\u001b[0m: DataFrame.drop() takes from 1 to 2 positional arguments but 3 were given"],"ename":"TypeError","evalue":"DataFrame.drop() takes from 1 to 2 positional arguments but 3 were given","output_type":"error"}]},{"cell_type":"code","source":"# Définissez la taille de groupe (100 lignes par document)\ngroup_size = 100\ndocuments = []\n\n# Initialiser un document en cours\ncurrent_document = \"\"\n\n# Parcourir les lignes de texte\nfor line in text:\n    current_document += line + \"\\n\"  # Ajouter la ligne actuelle au document en cours\n\n    # Si nous avons atteint le nombre de lignes souhaité, ajouter le document en cours à la liste des documents\n    if len(current_document.split('\\n')) >= group_size:\n        documents.append(current_document)\n        current_document = \"\"  # Réinitialiser le document en cours\n\n# Ajouter le dernier document si nécessaire (s'il n'est pas vide)\nif current_document:\n    documents.append(current_document)\n\n# Function to clean and process text (similar to your previous code)\ndef clean_text(text):\n    text_without_letter_apostrophe = re.sub(r\"\\b(?:[a-zA-Z])\\'\\b\", '', text)\n    # Tokenization (dividing the text into words)\n    words = nltk.word_tokenize(text_without_letter_apostrophe.lower())  # Convert to lowercase\n    # Remove punctuation, stopwords, and non-alphabetic words\n    cleaned_words = [word for word in words if word not in all_stopwords and word.isalpha()]\n    # Lemmatization\n    doc = nlp(\" \".join(cleaned_words))\n    lemmatized_words = [token.lemma_ for token in doc]\n    # Return a list of lemmatized words instead of a single string\n    return lemmatized_words\n\n\n# Traitement du texte ici\ncleanedTextList = [clean_text(doc) for doc in documents]\n# Convertissez votre texte nettoyé en une liste de listes de mots (tokens)\n#cleanedTextList = [cleanedText.split()]\n\n# Créez le dictionnaire à partir de la liste de listes de mots\ndictionary = gensim.corpora.Dictionary(cleanedTextList)\ndictionary.filter_extremes(no_below=15, no_above=0.95, keep_n=1000)\nprint(dictionary)\nbow_corpus = [dictionary.doc2bow(doc) for doc in cleanedTextList]\nlda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=100)\ntopics = []\nfor idx, topic in lda_model.print_topics(-1) :\n    print(\"Topic: {} -> Words: {}\".format(idx, topic))\n    topics.append(topic)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the necessary libraries\n#from flask import Flask, request, jsonify, abort\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport os\nimport re\nimport spacy\nimport numpy as np\nfrom PIL import Image\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\nnlp = spacy.load(\"fr_core_news_sm\")\nnlp.max_length = 13000000  # Augmentez cette valeur en fonction de la longueur de votre texte\n\n#app = Flask(__name__)\n\n# Load the French stopwords\nwith open('/kaggle/input/mydata/frenchStopwords2.txt', 'r', encoding='utf-8') as file:\n    frenchStopwords = file.read().split('\\n')\n\n# Combine French stopwords with NLTK stopwords\nall_stopwords = frenchStopwords + stopwords.words('french')\n\n\n# Define a folder for uploading files\n#UPLOAD_FOLDER = 'uploads'\n#if not os.path.exists(UPLOAD_FOLDER):\n#    os.makedirs(UPLOAD_FOLDER)\n\n# Read the text file\nwith open('/kaggle/input/mydata/text3.txt', 'r', encoding='utf-8') as file:\n    text = file.read().split('\\n')\n\n# Définissez la taille de groupe (100 lignes par document)\ngroup_size = 100\ndocuments = []\n\n# Initialiser un document en cours\ncurrent_document = \"\"\n\n# Parcourir les lignes de texte\nfor line in text:\n    current_document += line + \"\\n\"  # Ajouter la ligne actuelle au document en cours\n\n    # Si nous avons atteint le nombre de lignes souhaité, ajouter le document en cours à la liste des documents\n    if len(current_document.split('\\n')) >= group_size:\n        documents.append(current_document)\n        current_document = \"\"  # Réinitialiser le document en cours\n\n# Ajouter le dernier document si nécessaire (s'il n'est pas vide)\nif current_document:\n    documents.append(current_document)\n\n# Function to clean and process text (similar to your previous code)\ndef clean_text(text):\n    text_without_letter_apostrophe = re.sub(r\"\\b(?:[a-zA-Z])\\'\\b\", '', text)\n    # Tokenization (dividing the text into words)\n    words = nltk.word_tokenize(text_without_letter_apostrophe.lower())  # Convert to lowercase\n    # Remove punctuation, stopwords, and non-alphabetic words\n    cleaned_words = [word for word in words if word not in all_stopwords and word.isalpha()]\n    # Lemmatization\n    doc = nlp(\" \".join(cleaned_words))\n    lemmatized_words = [token.lemma_ for token in doc]\n    # Return a list of lemmatized words instead of a single string\n    return lemmatized_words\n\n\n# Traitement du texte ici\ncleanedTextList = [clean_text(doc) for doc in documents]\n# Convertissez votre texte nettoyé en une liste de listes de mots (tokens)\n#cleanedTextList = [cleanedText.split()]\n\n# Créez le dictionnaire à partir de la liste de listes de mots\ndictionary = gensim.corpora.Dictionary(cleanedTextList)\ndictionary.filter_extremes(no_below=15, no_above=0.99, keep_n=1000)\nprint(dictionary)\nbow_corpus = [dictionary.doc2bow(doc) for doc in cleanedTextList]\nlda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=100)\ntopics = []\nfor idx, topic in lda_model.print_topics(-1) :\n    print(\"Topic: {} -> Words: {}\".format(idx, topic))\n    topics.append(topic)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}